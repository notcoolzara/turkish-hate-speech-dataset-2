{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmnxqbIMiLwi",
        "outputId": "4682a441-2dff-4b80-c134-5fdb0a8a2a5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting zemberek-python\n",
            "  Downloading zemberek_python-0.2.3-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting antlr4-python3-runtime==4.8 (from zemberek-python)\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from zemberek-python) (1.26.4)\n",
            "Downloading zemberek_python-0.2.3-py3-none-any.whl (95.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.1/95.1 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141214 sha256=e2e3240c0d73cbd8e47a3d3563452b929ac428a354da72b4eec781bd1b09e20c\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, zemberek-python\n",
            "Successfully installed antlr4-python3-runtime-4.8 zemberek-python-0.2.3\n"
          ]
        }
      ],
      "source": [
        "!pip install zemberek-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EX_oi9T53BJg",
        "outputId": "2a794a9d-a8f3-49d3-fda0-4b9315eb4f7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.combine import SMOTEENN\n",
        "from gensim.models import KeyedVectors, Word2Vec\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from zemberek import TurkishMorphology, TurkishSpellChecker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeU4NWH73JDD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e88ce171-7cb7-4b6a-9397-e437b074a605"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "# Define paths\n",
        "dataset_path = '/content/drive/My Drive/DATASET/final_hate_speech.xlsx'\n",
        "word2vec_path = '/content/drive/My Drive/DATASET/word2vec_tr.model'\n",
        "fine_tuned_path = '/content/drive/My Drive/DATASET/word2vec_tr_finetuned.model'\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_excel(dataset_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTcu10Nb35dX",
        "outputId": "70e0339b-ecf0-44d7-bcd0-fa6fdfd775f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:zemberek.morphology.turkish_morphology:TurkishMorphology instance initialized in 10.18822169303894\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-01-05 14:03:25,446 - zemberek.morphology.turkish_morphology - INFO\n",
            "Msg: TurkishMorphology instance initialized in 10.18822169303894\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Initialize Zemberek for spell checking\n",
        "morphology = TurkishMorphology.create_with_defaults()\n",
        "spell_checker = TurkishSpellChecker(morphology)\n",
        "\n",
        "cache = {}\n",
        "\n",
        "def clean_turkish_text_with_cache(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
        "    text = ''.join([char for char in text if char.isalnum() or char.isspace()])  # Keep alphanumeric and spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
        "    corrected_words = []\n",
        "    for word in text.split():\n",
        "        if word in cache:\n",
        "            corrected_words.append(cache[word])\n",
        "        else:\n",
        "            suggestions = spell_checker.suggest_for_word(word)\n",
        "            correction = suggestions[0] if suggestions else word\n",
        "            cache[word] = correction\n",
        "            corrected_words.append(correction)\n",
        "    return ' '.join(corrected_words) if corrected_words else \"EMPTY\"\n",
        "\n",
        "# Apply preprocessing with caching\n",
        "data['tweet_cleaned'] = data['tweet'].apply(clean_turkish_text_with_cache)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmD4QJ1j7sxq"
      },
      "outputs": [],
      "source": [
        "# Tokenize cleaned tweets\n",
        "sentences = [tweet.split() for tweet in data['tweet_cleaned'] if tweet != \"EMPTY\"]\n",
        "\n",
        "# Load pre-trained Word2Vec model\n",
        "word2vec_model = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n",
        "\n",
        "# Fine-tune Word2Vec\n",
        "new_model = Word2Vec(vector_size=word2vec_model.vector_size, min_count=1)\n",
        "new_model.build_vocab(sentences)\n",
        "new_model.build_vocab([list(word2vec_model.key_to_index.keys())], update=True)\n",
        "new_model.wv.vectors = np.copy(word2vec_model.vectors)\n",
        "new_model.train(sentences, total_examples=len(sentences), epochs=10)\n",
        "new_model.save(fine_tuned_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MVmnBhagRkB"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGXQisEQ3-d9"
      },
      "outputs": [],
      "source": [
        "def text_to_word2vec(text, model, vector_size=300):\n",
        "    if not text or text.strip() == \"EMPTY\":\n",
        "        return np.zeros(vector_size, dtype=np.float32)\n",
        "    words = text.split()\n",
        "    embeddings = [model[word] for word in words if word in model.key_to_index]\n",
        "    if not embeddings:\n",
        "        return np.zeros(vector_size, dtype=np.float32)\n",
        "    return np.mean(embeddings, axis=0)\n",
        "\n",
        "X = np.array([text_to_word2vec(tweet, new_model.wv, vector_size=new_model.vector_size)\n",
        "             for tweet in data['tweet_cleaned']])\n",
        "y = data['etiket'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKNADF8iow9-"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define resampling methods\n",
        "resampling_methods = {\n",
        "    \"Original\": None,\n",
        "    \"Oversampling\": RandomOverSampler(random_state=42),\n",
        "    \"Undersampling\": RandomUnderSampler(random_state=42),\n",
        "    \"Combined\": SMOTEENN(random_state=42),\n",
        "}\n",
        "\n",
        "# Define ML models\n",
        "models = {\n",
        "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
        "    \"XGBoost\": XGBClassifier(random_state=42, tree_method='hist', use_label_encoder=False),\n",
        "    \"LightGBM\": LGBMClassifier(random_state=42),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJVsR2rf4Ek4"
      },
      "outputs": [],
      "source": [
        "# ANN Model\n",
        "def build_ann(input_dim):\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_dim,)),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(len(label_encoder.classes_), activation='softmax'),\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtO5Unpp4feB"
      },
      "outputs": [],
      "source": [
        "# Train and evaluate models\n",
        "results = []\n",
        "for res_name, resampler in resampling_methods.items():\n",
        "    if resampler:\n",
        "        X_resampled, y_resampled = resampler.fit_resample(X_train, y_train)\n",
        "    else:\n",
        "        X_resampled, y_resampled = X_train, y_train\n",
        "\n",
        "    for model_name, model in models.items():\n",
        "        model.fit(X_resampled, y_resampled)\n",
        "        y_pred = model.predict(X_test)\n",
        "        results.append({\n",
        "            \"Model\": model_name,\n",
        "            \"Resampling\": res_name,\n",
        "            \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "            \"Precision\": precision_score(y_test, y_pred, average='macro'),\n",
        "            \"Recall\": recall_score(y_test, y_pred, average='macro'),\n",
        "            \"F1-Score\": f1_score(y_test, y_pred, average='macro'),\n",
        "        })\n",
        "\n",
        "    # ANN\n",
        "    ann_model = build_ann(X_resampled.shape[1])\n",
        "    ann_model.fit(X_resampled, y_resampled, epochs=10, batch_size=32, verbose=0)\n",
        "    y_pred_ann = np.argmax(ann_model.predict(X_test), axis=1)\n",
        "    results.append({\n",
        "        \"Model\": \"ANN\",\n",
        "        \"Resampling\": res_name,\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred_ann),\n",
        "        \"Precision\": precision_score(y_test, y_pred_ann, average='macro'),\n",
        "        \"Recall\": recall_score(y_test, y_pred_ann, average='macro'),\n",
        "        \"F1-Score\": f1_score(y_test, y_pred_ann, average='macro'),\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2KE4EgUl4ig"
      },
      "outputs": [],
      "source": [
        "# Convert results to DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nTBfqagl7_D"
      },
      "outputs": [],
      "source": [
        "#SMOTEENN specific evaluation\n",
        "smote_enn = SMOTEENN(random_state=42)\n",
        "X_resampled, y_resampled = smote_enn.fit_resample(X_train, y_train)\n",
        "\n",
        "sampling_results = []\n",
        "for model_name, model in models.items():\n",
        "    model.fit(X_resampled, y_resampled)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    sampling_results.append({\n",
        "        \"Model\": model_name,\n",
        "        \"Sampling Type\": \"SMOTEENN\",\n",
        "        \"Accuracy\": accuracy,\n",
        "        \"Precision\": precision,\n",
        "        \"Recall\": recall,\n",
        "        \"F1-Score\": f1\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HD0iQYSl-YC"
      },
      "outputs": [],
      "source": [
        "sampling_df = pd.DataFrame(sampling_results)\n",
        "print(\"\\nDengeleme Yöntemleri ile Karşılaştırma:\")\n",
        "print(sampling_df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizations\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=results_df, x='Model', y='Accuracy', hue='Resampling')\n",
        "plt.title('Model Accuracy with Different Resampling Methods')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=results_df, x='Model', y='F1-Score', hue='Resampling')\n",
        "plt.title('Model F1-Score with Different Resampling Methods')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mneccqdaqH8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Results\n",
        "performance_results_path = '/content/drive/My Drive/DATASET/performance_results.csv'\n",
        "results_df.to_csv(performance_results_path, index=False)"
      ],
      "metadata": {
        "id": "SxCi7MXVqIjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nPerformance Results:\")\n",
        "print(results_df)"
      ],
      "metadata": {
        "id": "iJIzG2RAqK_i"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}